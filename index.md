---
layout: default
---

## Venue

The workshop will be collocated with [AACL 2020](http://aacl2020.org/).

## Important dates 

- July 3, 2020 - Call for papers released
- October 5, 2020 (Extended) – Paper submissions due
- September 21 - October 9, 2020 - Review period
- October 28, 2020 – Notification
- November 6, 2020 - Camera-ready due
- **December 7 (20:30-00:00 GMT+8 / 07:30-11:00 EST) [Join Zoom Meeting] **
 
## Workshop description

Machine learning for speech and language understanding tasks often strongly relies on large annotated data-sets to train the models. However, data collection and manual annotation is a time-consuming, expensive process often requiring a variety of bootstrapping methods to produce models that are "good enough". This slows down the development of new features and products.

The literature on bootstrapping ML systems often overlooks the constraints of real-world applications related to:

- annotation processes (examples are often annotated by batches instead of one by one);
- privacy (transfer learning from one language to another often requires to move data from one continent to another, which violates privacy policies);
- training times and resources;
- continual learning (introducing new classes but also merging or removing old ones).

The ability to efficiently move real-world systems to new domains and languages, or to adapt to changing conditions over time also often requires a complex mixture of techniques including active learning, transfer learning, continuous on-line learning, semi-supervised learning, and data augmentation as the models used by existing systems rarely generalize well to new circumstances. For example, current machine reading comprehension models do very well answering general, factoid style questions, but perform poorly on new specialized domains such as legal documents, operational manuals, financial policies, etc. Thus, domain transfer (especially from limited annotated data or using only unsupervised techniques) is needed to make the technology work for new scenarios. To address such issues, efforts for real-world applications need improved methods for targeting new use cases, features or classes. The approach also needs to be scalable to learn from both small limited-data sets at the beginning of a system’s life-cycle to larger data sets with millions of annotated data and/or billions of unannotated data as deployed systems expand to larger user bases and use cases.

In this workshop, we aim to cover challenges in a lifelong process where new users or functionalities are added, and existing functionalities are modified. We believe the challenge is prevalent in research from both academia and industry.


### Topics of Interest

- Semi-supervised learning
- Active learning
- Unsupervised learning
- Incremental learning
- Domain adaptation
- Data generation/augmentation
- Few shot learning
- Zero shot learning

LifeLongNLP 2020 is the second LifeLongNLP workshop. 
The programme and proceedings of the previous edition, which was held at ASRU 2019, can be found [here](https://sites.google.com/view/life-long-learning-asru19/).

The call for papers text is available [here](http://life-long-nlp.github.io/cfp).

## Organizers

- William M. Campbell,  Alexa AI, Amazon
- Alex Waibel, Carnegie Mellon University/Karlsruhe Institute of Technology
- Dilek Hakkani-Tur, Alexa AI, Amazon
- Timothy J. Hazen, Microsoft
- Kevin Kilgour, Google Research
- Eunah Cho, Alexa AI, Amazon
- Varun Kumar, Alexa AI, Amazon
- Hadrien Glaude, Alexa AI, Amazon

## Workshop program 

**Below time slots are in GMT+8** 
- Dec 7, 20:30-20:30 	Opening remarks 	
- Dec 7, 21:15-21:30 	invited talk - Haidar Khan (Alexa AI)
- Dec 7, 21:30-22:00 	"Paper 1: Deep Active Learning for Sequence Labeling Based on Diversity and Uncertainty in Gradient" - Yekyung Kim
- Dec 7, 22:00-22:30 	"Paper 2: Supervised Adaptation of Sequence-toSequence Speech Recognition Systems using BatchWeighting" -	Christian Huber, Juan Hussain, Tuan-Nam Nguyen, Kaihang Song, Sebastian Stüker and Alexander Waibel
- Dec 7, 22:30-22:45 	Break 	
- Dec 7, 23:45-23:30 	Invited talk -	Hadrien Glaude (Alexa AI)
- Dec 7, 23:30-24:00 	"Paper 3: Data Augmentation using Pre-trained Transformer Models” -	Varun Kumar, Ashutosh Choudhary and Eunah Cho
- Dec 7, 24:00-- 	Closing remarks 	

## Submission Guideline  

Please submit your paper using Softconf: [https://www.softconf.com/aacl-ijcnlp2020/LifeLongNLP/user/](https://www.softconf.com/aacl-ijcnlp2020/LifeLongNLP/user/)

**Format**: Submissions must be in PDF format, anonymized for review, written in English and follow the AACL2020 formatting requirements [http://aacl2020.org/calls/papers/#paper-submission-and-templates](http://aacl2020.org/calls/papers/#paper-submission-and-templates). 

**Length** : Submissions consist of up to eight pages of content. There is no limit on the number of pages for references. There is no extra space for appendices. There is no explicit short paper track, but you should feel free to submit your paper regardless of its length. Reviewers will be instructed not to penalize papers for being short.

**Dual Submission**: We do not allow dual submissions. Please refer to AACL guidelines. 

## Contact 

<life-long-nlp@googlegroups.com>
## Program committee
- Jan Niehues, Maastricht University
- Elizabeth Salesky,  CMU 
- Christian Federmann, Microsoft
- Abhyuday Jagannatha, UMass Amherst
- Beat Gfeller, Google Research
- Ankur Gandhe, Alexa Speech
- Hassan Rom, Google Research
- Matthias Sperber, Apple
- John Lalor, University of Notre Dame
- Teresa Herrmann, Echobot Media Technologies GmbH/Karlsruhe Institute of Technology
- Varun Nagaraja, Alexa Speech
- Nick Ruiz, Interactions LLC
- Krishna C. Puvvada, Alexa Speech
- Jervis Pinto, Electronic Arts
- Harsha Sundar, Alexa Speech
- Stan Peshterliev, Facebook
- Abhishek Sethi, Google
- Aditya Siddhant, Google 




## Anti-Harassment Policy
LifeLongNLP 2020 adheres to the [ACL Anti-Harassment Policy](https://www.aclweb.org/adminwiki/sphp?title=Anti-Harassment_Policy).
